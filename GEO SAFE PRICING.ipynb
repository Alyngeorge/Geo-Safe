{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ed98329-937f-4245-9d51-d63069a9373f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['NAME', 'Garden land without Road Access',\n",
      "       'Garden land with Road Access', 'Wet Land', 'Rocky Land',\n",
      "       'Residential Plot with Private Road Access',\n",
      "       'Residential Plot with NH/PWD Road Acess',\n",
      "       'Commercially Important Plot'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset (example: CSV file)\n",
    "df = pd.read_csv(\"plot_price_DS.csv\")\n",
    "\n",
    "# Print column names\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a41c7c-e86d-4777-9e9d-146ca6e73605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Set Performance:\n",
      "PRECIPITATION RATE: MAE = 0.0154, RMSE = 0.0220, R¬≤ = 0.9898\n",
      "SOIL MOISTURE: MAE = 0.0135, RMSE = 0.0173, R¬≤ = 0.9945\n",
      "WIND SPEED: MAE = 0.0134, RMSE = 0.0176, R¬≤ = 0.9950\n",
      "SURFACE TEMPERATURE: MAE = 0.0121, RMSE = 0.0168, R¬≤ = 0.9891\n",
      "DEEP SOIL TEMPERATURE: MAE = 0.0098, RMSE = 0.0137, R¬≤ = 0.9935\n",
      "\n",
      "Test Set Performance:\n",
      "PRECIPITATION RATE: MAE = 0.0175, RMSE = 0.0260, R¬≤ = 0.9862\n",
      "SOIL MOISTURE: MAE = 0.0131, RMSE = 0.0183, R¬≤ = 0.9942\n",
      "WIND SPEED: MAE = 0.0139, RMSE = 0.0189, R¬≤ = 0.9947\n",
      "SURFACE TEMPERATURE: MAE = 0.0120, RMSE = 0.0170, R¬≤ = 0.9891\n",
      "DEEP SOIL TEMPERATURE: MAE = 0.0105, RMSE = 0.0151, R¬≤ = 0.9916\n",
      "\n",
      "‚úÖ Future forecasts saved to 'future_forecast.csv'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"sorted_data.csv\")  # Update with your dataset path\n",
    "\n",
    "# Define target variables\n",
    "targets = ['PRECIPITATION RATE', 'SOIL MOISTURE', 'WIND SPEED',\n",
    "           'SURFACE TEMPERATURE', 'DEEP SOIL TEMPERATURE']\n",
    "\n",
    "# Define feature columns\n",
    "features = ['YEAR', 'MONTH', 'NAME', 'AVRG ELEVATION', 'MIN ELEVATION', 'MAX ELEVATION']\n",
    "\n",
    "# Split dataset (70% training, 10% validation, 20% testing)\n",
    "train_data, temp_data = train_test_split(df, test_size=0.3, shuffle=True, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=2/3, shuffle=True, random_state=42)\n",
    "\n",
    "# Extract input (X) and output (y)\n",
    "X_train, y_train = train_data[features], train_data[targets]\n",
    "X_val, y_val = val_data[features], val_data[targets]\n",
    "X_test, y_test = test_data[features], test_data[targets]\n",
    "\n",
    "# Define base models\n",
    "rf = RandomForestRegressor(n_estimators=150, random_state=42)\n",
    "xgb = XGBRegressor(n_estimators=150, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Create a stacking ensemble model wrapped in MultiOutputRegressor\n",
    "stack_model = MultiOutputRegressor(StackingRegressor(\n",
    "    estimators=[('rf', rf), ('xgb', xgb)],\n",
    "    final_estimator=LinearRegression()\n",
    "))\n",
    "\n",
    "# Train the model\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred = stack_model.predict(X_val)\n",
    "y_test_pred = stack_model.predict(X_test)\n",
    "\n",
    "# Convert predictions to DataFrame with aligned index\n",
    "y_val_pred_df = pd.DataFrame(y_val_pred, columns=targets, index=val_data.index)\n",
    "y_test_pred_df = pd.DataFrame(y_test_pred, columns=targets, index=test_data.index)\n",
    "\n",
    "# Save validation results for accuracy comparison\n",
    "validation_results = val_data[['YEAR', 'MONTH', 'NAME']].copy()\n",
    "for col in targets:\n",
    "    validation_results[f'Actual_{col}'] = y_val[col].values\n",
    "    validation_results[f'Predicted_{col}'] = y_val_pred_df[col].values\n",
    "\n",
    "validation_results.to_csv(\"validation_results.csv\", index=False)\n",
    "\n",
    "# Evaluation Metrics - Separate for Each Target\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "for col in targets:\n",
    "    mae = mean_absolute_error(y_val[col], y_val_pred_df[col])\n",
    "    rmse = np.sqrt(mean_squared_error(y_val[col], y_val_pred_df[col]))\n",
    "    r2 = r2_score(y_val[col], y_val_pred_df[col])\n",
    "    print(f\"{col}: MAE = {mae:.4f}, RMSE = {rmse:.4f}, R¬≤ = {r2:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "for col in targets:\n",
    "    mae = mean_absolute_error(y_test[col], y_test_pred_df[col])\n",
    "    rmse = np.sqrt(mean_squared_error(y_test[col], y_test_pred_df[col]))\n",
    "    r2 = r2_score(y_test[col], y_test_pred_df[col])\n",
    "    print(f\"{col}: MAE = {mae:.4f}, RMSE = {rmse:.4f}, R¬≤ = {r2:.4f}\")\n",
    "\n",
    "# Future Forecasting (Next 10 Years)\n",
    "future_years = list(range(df['YEAR'].max() + 1, df['YEAR'].max() + 11))\n",
    "future_data = []\n",
    "\n",
    "for year in future_years:\n",
    "    for month in range(1, 13):\n",
    "        for village in df['NAME'].unique():\n",
    "            avg_elevation = df[df['NAME'] == village]['AVRG ELEVATION'].mean()\n",
    "            min_elevation = df[df['NAME'] == village]['MIN ELEVATION'].mean()\n",
    "            max_elevation = df[df['NAME'] == village]['MAX ELEVATION'].mean()\n",
    "            future_data.append([year, month, village, avg_elevation, min_elevation, max_elevation])\n",
    "\n",
    "future_df = pd.DataFrame(future_data, columns=features)\n",
    "\n",
    "# Predict future values\n",
    "future_predictions = stack_model.predict(future_df)\n",
    "\n",
    "# Convert future predictions to DataFrame\n",
    "future_pred_df = future_df.copy()\n",
    "for i, col in enumerate(targets):\n",
    "    future_pred_df[col] = future_predictions[:, i]\n",
    "\n",
    "# Save future forecasts\n",
    "future_pred_df.to_csv(\"future_forecast.csv\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Future forecasts saved to 'future_forecast.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2d704-9419-46e2-a884-075dac44b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since elevation affects the severity of floods, landslides, and storms, we now include:\n",
    "\n",
    "    - Average Elevation (AE)\n",
    "    - Minimum Elevation (ME)\n",
    "    - Maximum Elevation (MAXE)\n",
    "\n",
    "1Ô∏è‚É£ Flood Risk Probability üåä\n",
    "PFlood = Œ±1‚ãÖPR + Œ±2‚ãÖSM ‚àí Œ±3‚ãÖST ‚àí Œ±4‚ãÖAE\n",
    "\n",
    "   - Higher AE ‚Üí Lower flood risk\n",
    "   - Higher PR & SM ‚Üí Higher flood risk\n",
    "   - Higher ST ‚Üí Lower flood risk\n",
    "\n",
    "2Ô∏è‚É£ Landslide Risk Probability üèîÔ∏è\n",
    "PLandslide = Œ≤1‚ãÖPR + Œ≤2‚ãÖSM + Œ≤3‚ãÖDST + Œ≤4‚ãÖAE\n",
    "\n",
    "   - Higher AE ‚Üí Higher landslide risk\n",
    "   - Higher PR, SM, DST ‚Üí Higher landslide risk\n",
    "   - Low AE but high MAXE ‚Üí Risky if slope is steep\n",
    "\n",
    "‚úÖ Slope Consideration:\n",
    "Slope = (MAXE ‚àí ME) / Distance\n",
    "\n",
    "   - If Slope > Threshold, landslide risk increases.\n",
    "\n",
    "3Ô∏è‚É£ Storm Risk Probability üå™Ô∏è\n",
    "PStorm = Œ≥1‚ãÖWS + Œ≥2‚ãÖPR ‚àí Œ≥3‚ãÖST ‚àí Œ≥4‚ãÖAE\n",
    "\n",
    "   - Higher AE ‚Üí Lower storm risk\n",
    "   - Higher WS & PR ‚Üí Higher storm risk\n",
    "   - Higher ST ‚Üí Lower storm risk\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0102f6f0-2d3c-44ff-a43d-141fce26fa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 09:58:34,800] A new study created in memory with name: no-name-b6eaeb3d-b3f4-45d6-8acc-b0cac5050341\n",
      "[I 2025-03-27 09:58:46,286] Trial 0 finished with value: 0.0002874434989635743 and parameters: {'n_estimators': 88, 'max_depth': 7, 'learning_rate': 0.22757013842328452, 'subsample': 0.8557415571618794, 'colsample_bytree': 0.8427391800923942}. Best is trial 0 with value: 0.0002874434989635743.\n",
      "[I 2025-03-27 09:58:58,223] Trial 1 finished with value: 0.00029300075148795603 and parameters: {'n_estimators': 142, 'max_depth': 6, 'learning_rate': 0.28686104491817904, 'subsample': 0.7281976236353602, 'colsample_bytree': 0.7182528151012229}. Best is trial 0 with value: 0.0002874434989635743.\n",
      "[I 2025-03-27 09:59:08,352] Trial 2 finished with value: 0.0008659217577103888 and parameters: {'n_estimators': 79, 'max_depth': 4, 'learning_rate': 0.12113330123394402, 'subsample': 0.9624738299034488, 'colsample_bytree': 0.982433697762769}. Best is trial 0 with value: 0.0002874434989635743.\n",
      "[I 2025-03-27 09:59:19,724] Trial 3 finished with value: 0.0008073115022773664 and parameters: {'n_estimators': 178, 'max_depth': 3, 'learning_rate': 0.2608239414890033, 'subsample': 0.8554424016659355, 'colsample_bytree': 0.7821680343923122}. Best is trial 0 with value: 0.0002874434989635743.\n",
      "[I 2025-03-27 09:59:32,017] Trial 4 finished with value: 0.0003459503059936735 and parameters: {'n_estimators': 75, 'max_depth': 9, 'learning_rate': 0.2404354469021454, 'subsample': 0.8156446817643348, 'colsample_bytree': 0.8562467553591939}. Best is trial 0 with value: 0.0002874434989635743.\n",
      "[I 2025-03-27 09:59:44,674] Trial 5 finished with value: 0.0002311394652028022 and parameters: {'n_estimators': 266, 'max_depth': 7, 'learning_rate': 0.2551562379138413, 'subsample': 0.9997963497644575, 'colsample_bytree': 0.761570039367657}. Best is trial 5 with value: 0.0002311394652028022.\n",
      "[I 2025-03-27 09:59:57,292] Trial 6 finished with value: 0.0002888570663434665 and parameters: {'n_estimators': 122, 'max_depth': 8, 'learning_rate': 0.23155725475409697, 'subsample': 0.6056080843500988, 'colsample_bytree': 0.7624857929888391}. Best is trial 5 with value: 0.0002311394652028022.\n",
      "[I 2025-03-27 10:00:08,181] Trial 7 finished with value: 0.0010786210196335239 and parameters: {'n_estimators': 205, 'max_depth': 3, 'learning_rate': 0.08180477458300907, 'subsample': 0.7794035350411699, 'colsample_bytree': 0.5797253210021871}. Best is trial 5 with value: 0.0002311394652028022.\n",
      "[I 2025-03-27 10:00:23,027] Trial 8 finished with value: 0.00024673108186373994 and parameters: {'n_estimators': 164, 'max_depth': 9, 'learning_rate': 0.1394073671793664, 'subsample': 0.6662420367957327, 'colsample_bytree': 0.6560255422668363}. Best is trial 5 with value: 0.0002311394652028022.\n",
      "[I 2025-03-27 10:00:39,472] Trial 9 finished with value: 0.000420267389819733 and parameters: {'n_estimators': 264, 'max_depth': 9, 'learning_rate': 0.06900502790614163, 'subsample': 0.7781532280595913, 'colsample_bytree': 0.5683863413299646}. Best is trial 5 with value: 0.0002311394652028022.\n",
      "[I 2025-03-27 10:00:53,428] Trial 10 finished with value: 0.0001969128440221099 and parameters: {'n_estimators': 295, 'max_depth': 5, 'learning_rate': 0.18889706224289343, 'subsample': 0.9943730065196695, 'colsample_bytree': 0.9742638785218154}. Best is trial 10 with value: 0.0001969128440221099.\n",
      "[I 2025-03-27 10:01:06,734] Trial 11 finished with value: 0.00019644045932612383 and parameters: {'n_estimators': 292, 'max_depth': 5, 'learning_rate': 0.1850965438234399, 'subsample': 0.9883350405726145, 'colsample_bytree': 0.9919930022568942}. Best is trial 11 with value: 0.00019644045932612383.\n",
      "[I 2025-03-27 10:01:20,762] Trial 12 finished with value: 0.00022682073847108517 and parameters: {'n_estimators': 298, 'max_depth': 5, 'learning_rate': 0.18098942294322268, 'subsample': 0.9261229537341302, 'colsample_bytree': 0.9848550127306452}. Best is trial 11 with value: 0.00019644045932612383.\n",
      "[I 2025-03-27 10:01:33,664] Trial 13 finished with value: 0.0003465980952618281 and parameters: {'n_estimators': 234, 'max_depth': 5, 'learning_rate': 0.1857523852983813, 'subsample': 0.5284074388986517, 'colsample_bytree': 0.9009017843175898}. Best is trial 11 with value: 0.00019644045932612383.\n",
      "[I 2025-03-27 10:01:47,806] Trial 14 finished with value: 0.0011028164427268011 and parameters: {'n_estimators': 296, 'max_depth': 5, 'learning_rate': 0.01122643371892254, 'subsample': 0.9043228472635132, 'colsample_bytree': 0.9302254576571709}. Best is trial 11 with value: 0.00019644045932612383.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_estimators': 292, 'max_depth': 5, 'learning_rate': 0.1850965438234399, 'subsample': 0.9883350405726145, 'colsample_bytree': 0.9919930022568942}\n",
      "Final MSE per target variable: [0.00029855 0.00014197 0.00019761 0.0001808  0.00016329]\n",
      "Average Final MSE: 0.00019644045932612383\n",
      "\n",
      "Validation Set Performance:\n",
      "PRECIPITATION RATE: MAE = 0.0154, RMSE = 0.0220, R¬≤ = 0.9898\n",
      "SOIL MOISTURE: MAE = 0.0135, RMSE = 0.0173, R¬≤ = 0.9945\n",
      "WIND SPEED: MAE = 0.0134, RMSE = 0.0176, R¬≤ = 0.9950\n",
      "SURFACE TEMPERATURE: MAE = 0.0121, RMSE = 0.0168, R¬≤ = 0.9891\n",
      "DEEP SOIL TEMPERATURE: MAE = 0.0098, RMSE = 0.0137, R¬≤ = 0.9935\n",
      "\n",
      "Test Set Performance:\n",
      "PRECIPITATION RATE: MAE = 0.0175, RMSE = 0.0260, R¬≤ = 0.9862\n",
      "SOIL MOISTURE: MAE = 0.0131, RMSE = 0.0183, R¬≤ = 0.9942\n",
      "WIND SPEED: MAE = 0.0139, RMSE = 0.0189, R¬≤ = 0.9947\n",
      "SURFACE TEMPERATURE: MAE = 0.0120, RMSE = 0.0170, R¬≤ = 0.9891\n",
      "DEEP SOIL TEMPERATURE: MAE = 0.0105, RMSE = 0.0151, R¬≤ = 0.9916\n",
      "\n",
      "‚úÖ Future forecasts saved to 'future_forecast.csv'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load your dataset (replace with actual data loading step)\n",
    "df = pd.read_csv(\"sorted_data.csv\")\n",
    "\n",
    "# Define features and target columns\n",
    "feature_columns = ['YEAR', 'MONTH', 'AVRG ELEVATION', 'MIN ELEVATION', 'MAX ELEVATION']\n",
    "target_columns = ['PRECIPITATION RATE', 'SOIL MOISTURE', 'WIND SPEED', 'SURFACE TEMPERATURE', 'DEEP SOIL TEMPERATURE']\n",
    "\n",
    "X = df[feature_columns]\n",
    "y = df[target_columns]  # This is multi-output (shape: [n_samples, 5])\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Optuna objective function\n",
    "def objective(trial):\n",
    "    # Hyperparameter tuning for XGBRegressor\n",
    "    xgb_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "    }\n",
    "\n",
    "    # Define base estimators\n",
    "    estimators = [\n",
    "        ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "        ('xgb', XGBRegressor(**xgb_params))\n",
    "    ]\n",
    "\n",
    "    # Use MultiOutputRegressor to handle multiple target variables\n",
    "    stack_model = MultiOutputRegressor(StackingRegressor(estimators=estimators, final_estimator=Ridge()))\n",
    "\n",
    "    # Train model\n",
    "    stack_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    y_val_pred = stack_model.predict(X_val)\n",
    "    \n",
    "    # Calculate mean squared error across all target variables\n",
    "    mse = mean_squared_error(y_val, y_val_pred, multioutput='raw_values')\n",
    "    return np.mean(mse)  # Optuna minimizes this value\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=15)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_xgb = XGBRegressor(**best_params)\n",
    "final_stack_model = MultiOutputRegressor(StackingRegressor(estimators=[\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "    ('xgb', best_xgb)\n",
    "], final_estimator=Ridge()))\n",
    "\n",
    "final_stack_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_test_pred = final_stack_model.predict(X_val)\n",
    "final_mse = mean_squared_error(y_val, y_test_pred, multioutput='raw_values')\n",
    "print(\"Final MSE per target variable:\", final_mse)\n",
    "print(\"Average Final MSE:\", np.mean(final_mse))\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"sorted_data.csv\")  # Update with your dataset path\n",
    "\n",
    "# Define target variables\n",
    "targets = ['PRECIPITATION RATE', 'SOIL MOISTURE', 'WIND SPEED',\n",
    "           'SURFACE TEMPERATURE', 'DEEP SOIL TEMPERATURE']\n",
    "\n",
    "# Define feature columns\n",
    "features = ['YEAR', 'MONTH', 'NAME', 'AVRG ELEVATION', 'MIN ELEVATION', 'MAX ELEVATION']\n",
    "\n",
    "# Split dataset (70% training, 10% validation, 20% testing)\n",
    "train_data, temp_data = train_test_split(df, test_size=0.3, shuffle=True, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=2/3, shuffle=True, random_state=42)\n",
    "\n",
    "# Extract input (X) and output (y)\n",
    "X_train, y_train = train_data[features], train_data[targets]\n",
    "X_val, y_val = val_data[features], val_data[targets]\n",
    "X_test, y_test = test_data[features], test_data[targets]\n",
    "\n",
    "# Define base models\n",
    "rf = RandomForestRegressor(n_estimators=150, random_state=42)\n",
    "xgb = XGBRegressor(n_estimators=150, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Create a stacking ensemble model wrapped in MultiOutputRegressor\n",
    "stack_model = MultiOutputRegressor(StackingRegressor(\n",
    "    estimators=[('rf', rf), ('xgb', xgb)],\n",
    "    final_estimator=LinearRegression()\n",
    "))\n",
    "\n",
    "# Train the model\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred = stack_model.predict(X_val)\n",
    "y_test_pred = stack_model.predict(X_test)\n",
    "\n",
    "# Convert predictions to DataFrame with aligned index\n",
    "y_val_pred_df = pd.DataFrame(y_val_pred, columns=targets, index=val_data.index)\n",
    "y_test_pred_df = pd.DataFrame(y_test_pred, columns=targets, index=test_data.index)\n",
    "\n",
    "# Save validation results for accuracy comparison\n",
    "validation_results = val_data[['YEAR', 'MONTH', 'NAME']].copy()\n",
    "for col in targets:\n",
    "    validation_results[f'Actual_{col}'] = y_val[col].values\n",
    "    validation_results[f'Predicted_{col}'] = y_val_pred_df[col].values\n",
    "\n",
    "validation_results.to_csv(\"validation_results.csv\", index=False)\n",
    "\n",
    "# Evaluation Metrics - Separate for Each Target\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "for col in targets:\n",
    "    mae = mean_absolute_error(y_val[col], y_val_pred_df[col])\n",
    "    rmse = np.sqrt(mean_squared_error(y_val[col], y_val_pred_df[col]))\n",
    "    r2 = r2_score(y_val[col], y_val_pred_df[col])\n",
    "    print(f\"{col}: MAE = {mae:.4f}, RMSE = {rmse:.4f}, R¬≤ = {r2:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "for col in targets:\n",
    "    mae = mean_absolute_error(y_test[col], y_test_pred_df[col])\n",
    "    rmse = np.sqrt(mean_squared_error(y_test[col], y_test_pred_df[col]))\n",
    "    r2 = r2_score(y_test[col], y_test_pred_df[col])\n",
    "    print(f\"{col}: MAE = {mae:.4f}, RMSE = {rmse:.4f}, R¬≤ = {r2:.4f}\")\n",
    "\n",
    "# Future Forecasting (Next 10 Years)\n",
    "future_years = list(range(df['YEAR'].max() + 1, df['YEAR'].max() + 11))\n",
    "future_data = []\n",
    "\n",
    "for year in future_years:\n",
    "    for month in range(1, 13):\n",
    "        for village in df['NAME'].unique():\n",
    "            avg_elevation = df[df['NAME'] == village]['AVRG ELEVATION'].mean()\n",
    "            min_elevation = df[df['NAME'] == village]['MIN ELEVATION'].mean()\n",
    "            max_elevation = df[df['NAME'] == village]['MAX ELEVATION'].mean()\n",
    "            future_data.append([year, month, village, avg_elevation, min_elevation, max_elevation])\n",
    "\n",
    "future_df = pd.DataFrame(future_data, columns=features)\n",
    "\n",
    "# Predict future values\n",
    "future_predictions = stack_model.predict(future_df)\n",
    "\n",
    "# Convert future predictions to DataFrame\n",
    "future_pred_df = future_df.copy()\n",
    "for i, col in enumerate(targets):\n",
    "    future_pred_df[col] = future_predictions[:, i]\n",
    "\n",
    "# Save future forecasts\n",
    "future_pred_df.to_csv(\"future_forecast.csv\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Future forecasts saved to 'future_forecast.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6125d1a-8162-4119-81a8-2e5e65a67938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Risk probabilities saved to 'risk_forecast.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load forecasted data\n",
    "forecast_df = pd.read_csv(\"future_forecast.csv\")\n",
    "\n",
    "# Coefficients for risk probabilities (these can be adjusted based on domain knowledge)\n",
    "alpha = [0.4, 0.3, 0.2, 0.1]  # Flood Risk\n",
    "beta = [0.3, 0.3, 0.2, 0.2]   # Landslide Risk\n",
    "gamma = [0.4, 0.3, 0.2, 0.1]  # Storm Risk\n",
    "\n",
    "# Define threshold for steep slope\n",
    "SLOPE_THRESHOLD = 0.2  # Adjust based on terrain severity\n",
    "\n",
    "def calculate_risk(row):\n",
    "    \"\"\"Calculate flood, landslide, and storm risk probabilities.\"\"\"\n",
    "    PR, SM, WS, ST, DST = row[['PRECIPITATION RATE', 'SOIL MOISTURE', 'WIND SPEED', 'SURFACE TEMPERATURE', 'DEEP SOIL TEMPERATURE']]\n",
    "    AE, ME, MAXE = row[['AVRG ELEVATION', 'MIN ELEVATION', 'MAX ELEVATION']]\n",
    "    \n",
    "    # Compute slope\n",
    "    slope = (MAXE - ME) / 1000  # Assuming distance ~1000m (adjust based on dataset)\n",
    "    landslide_risk = beta[0] * PR + beta[1] * SM + beta[2] * DST + beta[3] * AE\n",
    "    \n",
    "    # Increase landslide risk if slope is steep\n",
    "    if slope > SLOPE_THRESHOLD:\n",
    "        landslide_risk += 0.1 * slope  # Additional risk factor based on slope severity\n",
    "    \n",
    "    return pd.Series({\n",
    "        'Flood Risk': alpha[0] * PR + alpha[1] * SM - alpha[2] * ST - alpha[3] * AE,\n",
    "        'Landslide Risk': landslide_risk,\n",
    "        'Storm Risk': gamma[0] * WS + gamma[1] * PR - gamma[2] * ST - gamma[3] * AE\n",
    "    })\n",
    "\n",
    "# Apply risk calculations to each row\n",
    "risk_df = forecast_df.copy()\n",
    "risk_df[['Flood Risk', 'Landslide Risk', 'Storm Risk']] = forecast_df.apply(calculate_risk, axis=1)\n",
    "\n",
    "# Save results to CSV\n",
    "risk_df.to_csv(\"risk_forecast.csv\", index=False)\n",
    "print(\"\\n‚úÖ Risk probabilities saved to 'risk_forecast.csv'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5e911b-8563-46d5-88ee-e4053c3b2e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Disaster Susceptibility Index saved to 'disaster_susceptibility_index.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load risk forecast data\n",
    "risk_df = pd.read_csv(\"risk_forecast.csv\")\n",
    "\n",
    "# Define weights for different risks (adjustable)\n",
    "weights = {\n",
    "    'Flood Risk': 0.35,\n",
    "    'Landslide Risk': 0.30,\n",
    "    'Storm Risk': 0.25\n",
    "}\n",
    "\n",
    "# Normalize risk values per month and location\n",
    "scaler = MinMaxScaler()\n",
    "risk_df[['Flood Risk', 'Landslide Risk', 'Storm Risk']] = scaler.fit_transform(\n",
    "    risk_df[['Flood Risk', 'Landslide Risk', 'Storm Risk']]\n",
    ")\n",
    "\n",
    "# Compute Disaster Susceptibility Index (DSI)\n",
    "risk_df['Disaster Susceptibility Index'] = (\n",
    "    risk_df['Flood Risk'] * weights['Flood Risk'] +\n",
    "    risk_df['Landslide Risk'] * weights['Landslide Risk'] +\n",
    "    risk_df['Storm Risk'] * weights['Storm Risk']\n",
    ")\n",
    "\n",
    "# Group by Year, Month, and Location (NAME) to get average DSI per location per month\n",
    "dsi_df = risk_df.groupby(['YEAR', 'MONTH', 'NAME'], as_index=False)['Disaster Susceptibility Index'].mean()\n",
    "\n",
    "# Save results\n",
    "dsi_df.to_csv(\"disaster_susceptibility_index.csv\", index=False)\n",
    "print(\"\\n‚úÖ Disaster Susceptibility Index saved to 'disaster_susceptibility_index.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb2ad77-727e-49a5-b4d0-4ae4fdd24b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Merged dataset saved as 'merged_risk_dsi.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Risk Forecast Data\n",
    "risk_df = pd.read_csv(\"risk_forecast.csv\")\n",
    "\n",
    "# Load Disaster Susceptibility Index Data\n",
    "dsi_df = pd.read_csv(\"disaster_susceptibility_index.csv\")\n",
    "\n",
    "# Merge on YEAR, MONTH, NAME\n",
    "merged_df = pd.merge(risk_df, dsi_df, on=['YEAR', 'MONTH', 'NAME'], how='left')\n",
    "\n",
    "# Save merged dataset\n",
    "merged_df.to_csv(\"merged_risk_dsi.csv\", index=False)\n",
    "print(\"\\n‚úÖ Merged dataset saved as 'merged_risk_dsi.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca768ce-d85b-4863-b897-de43af525514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ad6b5e7-6871-4763-8d1f-3780b7ebf456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in risk_dsi_df: Index(['YEAR', 'MONTH', 'NAME', 'AVRG ELEVATION', 'MIN ELEVATION',\n",
      "       'MAX ELEVATION', 'PRECIPITATION RATE', 'SOIL MOISTURE', 'WIND SPEED',\n",
      "       'SURFACE TEMPERATURE', 'DEEP SOIL TEMPERATURE', 'Flood Risk',\n",
      "       'Landslide Risk', 'Storm Risk', 'Disaster Susceptibility Index'],\n",
      "      dtype='object')\n",
      "Columns in plot_price_df: Index(['NAME', 'Garden land without Road Access',\n",
      "       'Garden land with Road Access', 'Wet Land', 'Rocky Land',\n",
      "       'Residential Plot with Private Road Access',\n",
      "       'Residential Plot with NH/PWD Road Acess',\n",
      "       'Commercially Important Plot'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "risk_dsi_df = pd.read_csv(\"merged_risk_dsi.csv\")\n",
    "plot_price_df = pd.read_csv(\"plot_price_DS_2.csv\")\n",
    "\n",
    "# Print column names to check for 'YEAR', 'MONTH', 'NAME'\n",
    "print(\"Columns in risk_dsi_df:\", risk_dsi_df.columns)\n",
    "print(\"Columns in plot_price_df:\", plot_price_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5793afe3-28c7-4320-9e52-f47e0552fcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alyng\\AppData\\Local\\Temp\\ipykernel_20068\\777006388.py:27: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  plot_price_df = plot_price_df.groupby('NAME', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Future plot prices with DSI (with mapped names) generated successfully: future_plot_prices_with_DSI_2024_2034.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "risk_dsi_df = pd.read_csv('merged_risk_dsi.csv')  # Contains yearly & monthly DSI\n",
    "plot_price_df = pd.read_csv('plot_price_DS_2.csv')  # Contains initial plot prices\n",
    "\n",
    "# Mapping for numerical NAME values to their respective place names\n",
    "name_mapping = {\n",
    "    0.0625: \"Alakode\", 0.125: \"Arakkulam\", 0.1875: \"Elappally\",\n",
    "    0.25: \"Kanjikuzhy\", 0.3125: \"Karimkunnam\", 0.375: \"Karimannoor\",\n",
    "    0.4375: \"Kodikkulam\", 0.5: \"Kudayathoor\", 0.5625: \"Kumaramangalam\",\n",
    "    0.625: \"Manakkad\", 0.6875: \"Muttom\", 0.75: \"Purapuzha\",\n",
    "    0.8125: \"Udumbannoor\", 0.875: \"Vannappuram\", 0.9375: \"Velliyamattom\",\n",
    "    1.0: \"Keerikode\"\n",
    "}\n",
    "\n",
    "# Convert NAME to numeric (if stored as a string)\n",
    "risk_dsi_df['NAME'] = risk_dsi_df['NAME'].astype(float)\n",
    "plot_price_df['NAME'] = plot_price_df['NAME'].astype(float)\n",
    "\n",
    "# Convert plot price columns to numeric\n",
    "for col in plot_price_df.columns[1:]:\n",
    "    plot_price_df[col] = pd.to_numeric(plot_price_df[col], errors='coerce').replace(0, np.nan)\n",
    "\n",
    "# Step 1: Fill NaNs using interpolation for each NAME\n",
    "plot_price_df = plot_price_df.groupby('NAME', group_keys=False).apply(\n",
    "    lambda group: group.interpolate(method='linear', limit_direction='both')\n",
    ")\n",
    "\n",
    "# Step 2: Fill remaining NaNs using median values\n",
    "plot_price_df.fillna(plot_price_df.median(), inplace=True)\n",
    "\n",
    "# Define yearly inflation rates (2024-2034)\n",
    "inflation_rates = {\n",
    "    2024: 1.048, 2025: 1.043, 2026: 1.042,\n",
    "    2027: 1.041, 2028: 1.040, 2029: 1.039,\n",
    "    2030: 1.038, 2031: 1.037, 2032: 1.036,\n",
    "    2033: 1.035, 2034: 1.034\n",
    "}\n",
    "\n",
    "# Convert 'YEAR' and 'MONTH' to numeric\n",
    "risk_dsi_df['YEAR'] = risk_dsi_df['YEAR'].astype(int)\n",
    "risk_dsi_df['MONTH'] = risk_dsi_df['MONTH'].astype(int)\n",
    "\n",
    "# Function to adjust plot prices based on DSI & inflation\n",
    "def adjust_prices(name, year, month, base_prices, dsi_value):\n",
    "    \"\"\"Applies disaster susceptibility index (DSI) adjustment & inflation.\"\"\"\n",
    "    \n",
    "    adjustment_factor = 1 / (1 + dsi_value)  # Inverse relation with DSI\n",
    "    inflation_factor = np.prod([inflation_rates[y] for y in range(2024, year + 1)])  # Inflation over time\n",
    "\n",
    "    new_prices = {}\n",
    "    for col, price_value in base_prices.items():\n",
    "        if pd.notna(price_value):  # Ignore NaNs\n",
    "            new_prices[col] = float(price_value) * adjustment_factor * inflation_factor\n",
    "        else:\n",
    "            new_prices[col] = np.nan  # Keep structure\n",
    "    return new_prices\n",
    "\n",
    "# Create a single DataFrame to store all results\n",
    "all_data = []\n",
    "\n",
    "# Generate dataset for each month from 2024-2034\n",
    "for year in range(2024, 2035):\n",
    "    for month in range(1, 13):  # Loop through all months\n",
    "        \n",
    "        # Filter DSI values for the current year and month\n",
    "        dsi_filtered = risk_dsi_df[(risk_dsi_df['YEAR'] == year) & (risk_dsi_df['MONTH'] == month)]\n",
    "        \n",
    "        for _, row in dsi_filtered.iterrows():\n",
    "            name_num = row['NAME']\n",
    "            dsi_value = row['Disaster Susceptibility Index']\n",
    "            \n",
    "            # Get the corresponding place name\n",
    "            name_str = name_mapping.get(name_num, f\"Unknown_{name_num}\")\n",
    "\n",
    "            # Extract base plot prices for NAME\n",
    "            base_prices = plot_price_df[plot_price_df['NAME'] == name_num].iloc[:, 1:].squeeze()\n",
    "\n",
    "            if base_prices.empty:\n",
    "                print(f\"‚ö†Ô∏è No base prices found for NAME: {name_str} ({name_num}) in {year}-{month}!\")\n",
    "                continue  # Skip missing data\n",
    "            \n",
    "            new_prices = adjust_prices(name_num, year, month, base_prices.to_dict(), dsi_value)\n",
    "\n",
    "            new_row = {'NAME': name_str, 'YEAR': year, 'MONTH': month, 'DSI': dsi_value, **new_prices}\n",
    "            all_data.append(new_row)\n",
    "\n",
    "# Convert to a single DataFrame\n",
    "final_df = pd.DataFrame(all_data)\n",
    "\n",
    "# Save to a single CSV file\n",
    "final_df.to_csv('future_plot_prices_with_DSI_2024_2034.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Future plot prices with DSI (with mapped names) generated successfully: future_plot_prices_with_DSI_2024_2034.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0be0c-7ed8-4e45-ac57-c0109586dd91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
